{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "qnn",
   "display_name": "qnn",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "from pennylane import numpy as np\n",
    "from scipy.linalg import expm\n",
    "\n",
    "from math import pi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.29009404 0.1560478  0.         0.20826707 0.13447822]\n [0.         0.35103159 0.1948876  0.35325602 0.34571177]\n [0.28993302 0.15662525 0.13062393 0.22779281 0.13663156]\n [0.28999742 0.07591331 0.04451761 0.08843148 0.07069606]\n [0.28996522 0.29643381 0.22690455 0.30726691 0.2870652 ]]\ntensor([961., 154., 905.,  ...,  57., 311., 867.])\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "devSet = pd.read_csv(\"./us_migration.csv\")\n",
    "devSet = devSet.loc[:, ~devSet.columns.str.contains('^Unnamed')]\n",
    "devSet = devSet.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "devSet = devSet.dropna(axis=1)\n",
    "\n",
    "y = torch.Tensor(devSet['US_MIG_05_10'].values)\n",
    "X = devSet.loc[:, devSet.columns != \"US_MIG_05_10\"].values\n",
    "\n",
    "mMScale = preprocessing.MinMaxScaler()\n",
    "X = mMScale.fit_transform(X)\n",
    "\n",
    "indices = random.sample(range(0, 10), 5)\n",
    "\n",
    "\n",
    "# x1 = np.reshape(X[0][0:5], (1, 5))\n",
    "# x2 = np.reshape(X[1][0:5], (1, 5))\n",
    "# x3 = np.reshape(X[2][0:5], (1, 5))\n",
    "x = np.array([list(i)[0:5] for i in X[random.sample(range(0, 10), 5)]])\n",
    "# x = np.concatenate([x1, x2, x3])\n",
    "# y = torch.tensor(y.detach().numpy()[[8, 5, 6, 2, 4]])\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX = qml.device('default.qubit', wires = len(x[0]))\n",
    "devY = qml.device('default.qubit', wires = len(x[0]))\n",
    "devZ = qml.device('default.qubit', wires = len(x[0]))\n",
    "\n",
    "@qml.qnode(devX)\n",
    "def rotation_circuitX(vals, thetas):\n",
    "\n",
    "    # Apply Hadamards\n",
    "    for hadamard_wire in range(len(vals)):\n",
    "        qml.Hadamard(wires = hadamard_wire)\n",
    "\n",
    "    # Apply value dependent Z-axis rotations\n",
    "    for rotation_val in range(len(vals)):\n",
    "        qml.RZ(vals[rotation_val], wires = rotation_val)\n",
    "\n",
    "    # Random CNOT\n",
    "    qml.CNOT(wires = [1,0])\n",
    "\n",
    "    # Parametized rotation <- this is what is being trained\n",
    "    for theta in range(len(thetas)):\n",
    "        qml.RX(thetas[theta], wires=theta)\n",
    "\n",
    "    # Get expected values & return them\n",
    "    expected_values = [qml.expval(qml.PauliX(wire)) for wire in range(len(vals))]\n",
    "    return expected_values\n",
    "\n",
    "\n",
    "@qml.qnode(devY)\n",
    "def rotation_circuitY(vals, thetas):\n",
    "\n",
    "    # Apply Hadamards\n",
    "    for hadamard_wire in range(len(vals)):\n",
    "        qml.Hadamard(wires = hadamard_wire)\n",
    "\n",
    "    # Apply value dependent Z-axis rotations\n",
    "    for rotation_val in range(len(vals)):\n",
    "        qml.RZ(vals[rotation_val], wires = rotation_val)\n",
    "\n",
    "    # Random CNOT\n",
    "    qml.CNOT(wires = [1,0])\n",
    "\n",
    "    # Parametized rotation <- this is what is being trained\n",
    "    for theta in range(len(thetas)):\n",
    "        qml.RY(thetas[theta], wires=theta)\n",
    "\n",
    "    # Get expected values & return them\n",
    "    expected_values = [qml.expval(qml.PauliY(wire)) for wire in range(len(vals))]\n",
    "    return expected_values\n",
    "\n",
    "\n",
    "@qml.qnode(devZ)\n",
    "def rotation_circuitZ(vals, thetas):\n",
    "\n",
    "    # Apply Hadamards\n",
    "    for hadamard_wire in range(len(vals)):\n",
    "        qml.Hadamard(wires = hadamard_wire)\n",
    "\n",
    "    # Apply value dependent Z-axis rotations\n",
    "    for rotation_val in range(len(vals)):\n",
    "        qml.RZ(vals[rotation_val], wires = rotation_val)\n",
    "\n",
    "    # Random CNOT\n",
    "    qml.CNOT(wires = [1,0])\n",
    "\n",
    "    # Parametized rotation <- this is what is being trained\n",
    "    for theta in range(len(thetas)):\n",
    "        qml.RZ(thetas[theta], wires=theta)\n",
    "\n",
    "    # Get expected values & return them\n",
    "    expected_values = [qml.expval(qml.PauliZ(wire)) for wire in range(len(vals))]\n",
    "    return expected_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Define our model\n",
    "class QuantumCicuitNet(torch.nn.Module):\n",
    "    def __init__(self, n_vals, n_dim, batch_size):\n",
    "        super().__init__()\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.conv2d = torch.nn.Conv2d(1, 1, kernel_size=(3,3), stride=(2,2), padding=(2,2), bias=False)\n",
    "        self.linear = torch.nn.Linear(16, n_dim)  \n",
    "\n",
    "\n",
    "    def param_shift(self, vals, thetas, axis):\n",
    "        # using the convention u=1/2\n",
    "        if axis == 'X':\n",
    "            r_plus = rotation_circuitX(vals, np.array(thetas) + np.array(np.pi / 2))\n",
    "            r_minus = rotation_circuitX(vals, np.array(thetas) - np.array(np.pi / 2))\n",
    "            return 10 * (r_plus - r_minus)\n",
    "        elif axis == 'Y':\n",
    "            r_plus = rotation_circuitY(vals, np.array(thetas) + np.array(np.pi / 2))\n",
    "            r_minus = rotation_circuitY(vals, np.array(thetas) - np.array(np.pi / 2))\n",
    "            return 10 * (r_plus - r_minus)\n",
    "        elif axis == 'Z':\n",
    "            r_plus = rotation_circuitZ(vals, np.array(thetas) + np.array(np.pi / 2))\n",
    "            r_minus = rotation_circuitZ(vals, np.array(thetas) - np.array(np.pi / 2))\n",
    "            return .5 * (r_plus - r_minus)\n",
    "        \n",
    "\n",
    "    def run_circs(self, x, thetas):\n",
    "        outs = []\n",
    "        for i in range(0, x.shape[0]):\n",
    "            outX = torch.tensor(rotation_circuitX(x[i], thetas), dtype = torch.float32) * 100# OUT:  torch.Size([100, 1, 10, 10])\n",
    "            outY = torch.tensor(rotation_circuitY(x[i], thetas), dtype = torch.float32) * 100# OUT:  torch.Size([100, 1, 10, 10])\n",
    "            outZ = torch.tensor(rotation_circuitZ(x[i], thetas), dtype = torch.float32) * 100# OUT:  torch.Size([100, 1, 10, 10])\n",
    "            outs.append(torch.reshape(torch.cat((outX, outY,outZ), 0), (3, 5)))\n",
    "        return torch.reshape(torch.cat(outs), (x.shape[0], 3, x.shape[1], 1))\n",
    "\n",
    "    def calc_grads(self, x, thetas):\n",
    "        all_grads = []\n",
    "        for i in range(0, x.shape[0]):\n",
    "            gradX = torch.tensor(self.param_shift(x[i], thetas, 'X'))\n",
    "            gradY = torch.tensor(self.param_shift(x[i], thetas, 'Y'))\n",
    "            gradZ = torch.tensor(self.param_shift(x[i], thetas, 'Z'))\n",
    "            grads = torch.reshape(torch.cat((gradX, gradY,gradZ), 0), (3,5))\n",
    "            grads = torch.mean(input = grads, dim = 0)\n",
    "            all_grads.append(list(grads.detach().numpy()))\n",
    "        \n",
    "        \n",
    "        # print(\"Gradients: \", torch.mean(input = torch.tensor(all_grads), dim = 0))\n",
    "        return torch.mean(input = torch.tensor(all_grads), dim = 0)\n",
    "\n",
    "        \n",
    "    def forward(self, x, thetas):\n",
    "        # out = self.run_circs(x, thetas)\n",
    "        # grads = self.calc_grads(x, thetas)\n",
    "        out = self.conv2d(x)\n",
    "        out = self.relu(out)\n",
    "        out = out.flatten()\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(real, pred):\n",
    "    '''\n",
    "    Calculates MAE of an epoch\n",
    "    '''\n",
    "    return torch.abs(real - pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH:  0\n",
      "    Loss:  tensor(980850.9375, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(341.4768, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([203.,   0., 654., 151., 699.]) Y Pred:  tensor([ 0.1424,  0.1497, -0.1938,  0.0659, -0.2489], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  1\n",
      "    Loss:  tensor(597574.9375, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(295.5659, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([245., 479., 527.,  65., 162.]) Y Pred:  tensor([ 0.1760,  0.1066, -0.0716,  0.0550, -0.0952], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  2\n",
      "    Loss:  tensor(263305.9062, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(177.4821, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 93., 460., 154., 131.,  50.]) Y Pred:  tensor([ 0.2461,  0.2578,  0.0509,  0.1122, -0.0774], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  3\n",
      "    Loss:  tensor(1934411., grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(594.8536, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([692., 500., 735., 766., 282.]) Y Pred:  tensor([ 0.2584,  0.3251,  0.0914,  0.1128, -0.0561], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  4\n",
      "    Loss:  tensor(926224.2500, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(306.1292, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([559.,  41., 775.,  80.,  77.]) Y Pred:  tensor([0.3939, 0.4265, 0.2385, 0.2817, 0.0132], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  5\n",
      "    Loss:  tensor(624662.6250, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(313.8669, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([144., 494., 194., 528., 211.]) Y Pred:  tensor([0.5268, 0.4375, 0.3920, 0.2910, 0.0181], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  6\n",
      "    Loss:  tensor(432952.2188, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(198.6015, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([  9., 613.,  43., 194., 136.]) Y Pred:  tensor([0.5366, 0.5317, 0.4292, 0.4029, 0.0919], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  7\n",
      "    Loss:  tensor(1801633.3750, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(443.7257, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 309.,  200., 1240.,  138.,  334.]) Y Pred:  tensor([0.5670, 0.7093, 0.4870, 0.5088, 0.0995], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  8\n",
      "    Loss:  tensor(2688288.7500, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(376.8755, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([  55., 1634.,   10.,   98.,   90.]) Y Pred:  tensor([0.5935, 0.7026, 0.6884, 0.4763, 0.1619], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  9\n",
      "    Loss:  tensor(231122.2344, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(184.1632, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([282.,   3., 255., 109., 275.]) Y Pred:  tensor([0.6574, 1.1172, 0.7230, 0.5407, 0.1458], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  10\n",
      "    Loss:  tensor(301631.9062, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(185.0861, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([198., 125.,  64.,  51., 491.]) Y Pred:  tensor([0.7042, 1.1494, 0.8386, 0.6329, 0.2444], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  11\n",
      "    Loss:  tensor(392104.5625, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(178.7470, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([179.,   0.,  44.,  78., 594.]) Y Pred:  tensor([0.7565, 1.1807, 0.7946, 0.5891, 0.3054], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  12\n",
      "    Loss:  tensor(445434.5000, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(241.4067, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 10., 553., 250., 194., 204.]) Y Pred:  tensor([0.7339, 1.1933, 0.8331, 0.6647, 0.5416], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  13\n",
      "    Loss:  tensor(1791754.2500, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(372.7015, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 591.,   34.,    0., 1201.,   40.]) Y Pred:  tensor([0.7908, 1.3566, 0.8521, 0.6739, 0.5236], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  14\n",
      "    Loss:  tensor(1554813.2500, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(392.2497, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([1034.,  651.,   19.,    6.,  256.]) Y Pred:  tensor([0.9751, 1.3156, 0.9275, 0.9364, 0.5970], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  15\n",
      "    Loss:  tensor(375289.8438, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(255.7078, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([353., 342., 175., 308., 106.]) Y Pred:  tensor([1.2752, 1.4788, 1.0122, 0.9630, 0.7319], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  16\n",
      "    Loss:  tensor(40964.8398, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(72.6510, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 16.,  26., 153.,  54., 120.]) Y Pred:  tensor([1.3755, 1.6585, 0.9610, 1.0985, 0.6516], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  17\n",
      "    Loss:  tensor(1421339.2500, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(418.1844, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([957., 373., 126., 595.,  46.]) Y Pred:  tensor([1.3472, 1.7252, 1.0473, 1.2332, 0.7253], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  18\n",
      "    Loss:  tensor(646005.6875, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(312.0322, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([545., 330., 415., 268.,   8.]) Y Pred:  tensor([1.4796, 1.5486, 0.9617, 1.1097, 0.7395], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  19\n",
      "    Loss:  tensor(471076.7188, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(228.2050, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([350.,  86., 579.,  55.,  78.]) Y Pred:  tensor([1.7981, 1.9392, 1.1422, 1.3428, 0.7525], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  20\n",
      "    Loss:  tensor(291001.0938, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(211.4660, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([193., 131., 274.,  67., 400.]) Y Pred:  tensor([2.0506, 1.9645, 1.4599, 1.3145, 0.8803], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  21\n",
      "    Loss:  tensor(61869.6289, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(91.0288, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 37.,   8., 116., 115., 187.]) Y Pred:  tensor([1.9517, 1.9463, 1.4936, 1.5097, 0.9548], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  22\n",
      "    Loss:  tensor(607196.5000, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(261.9834, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([462., 118.,  72.,  53., 613.]) Y Pred:  tensor([1.9640, 2.0086, 1.4897, 1.5797, 1.0411], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  23\n",
      "    Loss:  tensor(553808.3750, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(302.6336, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([396., 530., 236., 206., 154.]) Y Pred:  tensor([2.2282, 2.0471, 1.6690, 1.5368, 1.3512], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  24\n",
      "    Loss:  tensor(302858.0938, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(199.1983, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([111., 454., 170.,  28., 242.]) Y Pred:  tensor([2.3605, 2.2558, 1.6494, 1.4403, 1.3023], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  25\n",
      "    Loss:  tensor(229683.6562, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(187.8045, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([314., 120., 266.,  28., 223.]) Y Pred:  tensor([2.9505, 3.1622, 2.0118, 2.0232, 1.8298], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  26\n",
      "    Loss:  tensor(322344.5625, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(176.0878, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([522.,  38.,  64., 216.,  54.]) Y Pred:  tensor([3.4086, 3.2928, 2.5047, 2.1292, 2.2257], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  27\n",
      "    Loss:  tensor(399494.8125, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(238.9497, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([507., 183.,  41., 257., 222.]) Y Pred:  tensor([4.1390, 3.5327, 2.5812, 2.6421, 2.3567], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  28\n",
      "    Loss:  tensor(459849.7812, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(157.0359, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 28.,  25.,  31., 679.,  40.]) Y Pred:  tensor([5.0632, 4.0675, 2.7483, 3.2051, 2.7364], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  29\n",
      "    Loss:  tensor(209841.7188, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(158.6305, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([329., 247.,  12., 219.,   0.]) Y Pred:  tensor([5.3201, 4.4276, 2.7958, 4.2010, 2.8973], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  30\n",
      "    Loss:  tensor(614493.1875, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(288.6307, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([459., 298.,  68., 566.,  75.]) Y Pred:  tensor([6.3356, 4.9986, 2.9938, 5.3251, 3.1935], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  31\n",
      "    Loss:  tensor(332827.9062, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(212.2467, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 98., 153.,  82., 478., 274.]) Y Pred:  tensor([6.6805, 5.1972, 2.9968, 5.7070, 3.1849], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  32\n",
      "    Loss:  tensor(685476.9375, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(272.2236, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([466., 653.,  16., 258.,   5.]) Y Pred:  tensor([ 9.9498,  7.9034,  3.6855, 10.6262,  4.7169], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  33\n",
      "    Loss:  tensor(77985.9766, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(108.3612, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 99., 242., 114.,  93.,  56.]) Y Pred:  tensor([17.3388, 15.3658,  5.9932, 15.8732,  7.6231], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  34\n",
      "    Loss:  tensor(686447.5625, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(340.8481, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([229., 224., 376., 314., 612.]) Y Pred:  tensor([13.8024, 13.4689,  5.1747, 12.3995,  5.9143], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  35\n",
      "    Loss:  tensor(1674132., grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(461.3975, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([1123.,  333.,  508.,   98.,  307.]) Y Pred:  tensor([15.3158, 14.9778,  7.3548, 14.7853,  9.5790], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  36\n",
      "    Loss:  tensor(1514638.5000, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(270.5642, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([  28.,   91., 1240.,   65.,   18.]) Y Pred:  tensor([25.7412, 19.9399, 12.2776, 17.4197, 13.8005], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  37\n",
      "    Loss:  tensor(378246.6562, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(237.6129, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([503., 270., 160., 320.,  89.]) Y Pred:  tensor([38.7592, 30.8974, 36.3247, 27.4849, 20.4693], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  38\n",
      "    Loss:  tensor(536320.3750, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(275.3459, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([225., 566., 345., 418.,  32.]) Y Pred:  tensor([56.4727, 41.7127, 46.1388, 38.9382, 26.0083], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  39\n",
      "    Loss:  tensor(403426.6562, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(210.8768, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([537., 120., 481.,   5.,  91.]) Y Pred:  tensor([69.1276, 64.8842, 62.6080, 56.6240, 29.6203], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  40\n",
      "    Loss:  tensor(330702.8750, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(137.6272, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([105.,   0., 669.,  57.,  50.]) Y Pred:  tensor([113.9087,  86.8866, 100.8682,  71.0356,  39.8268],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  41\n",
      "    Loss:  tensor(1671805.8750, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(388.2443, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([  68., 1178.,  185.,    5.,  728.]) Y Pred:  tensor([118.6926,  86.5463, 142.6918,  75.3035,  41.5364],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  42\n",
      "    Loss:  tensor(463859.8750, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(187.3114, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([163., 344., 108., 136., 784.]) Y Pred:  tensor([151.0299, 216.8300, 198.0217,  89.9029, 122.7016],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  43\n",
      "    Loss:  tensor(45831.8203, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(90.9139, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 75., 155.,  76.,  41., 284.]) Y Pred:  tensor([171.6938, 265.1583, 208.3991, 107.8405, 235.5221],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  44\n",
      "    Loss:  tensor(127306.1172, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(135.1884, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([233., 311., 427., 100., 354.]) Y Pred:  tensor([128.1525, 199.1949, 148.8361,  80.2693, 192.6052],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  45\n",
      "    Loss:  tensor(70872.8594, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(107.0882, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([302., 346.,  26.,  30., 175.]) Y Pred:  tensor([140.9997, 213.1744, 178.0472,  84.2593, 210.3088],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  46\n",
      "    Loss:  tensor(131319.7812, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(136.0272, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([106., 566., 144.,  45.,  75.]) Y Pred:  tensor([196.7991, 283.1418, 208.5733,  96.5347, 265.3706],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  47\n",
      "    Loss:  tensor(169476.2812, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(145.9370, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([129., 187., 464., 300., 283.]) Y Pred:  tensor([137.2848, 254.0309, 148.9556,  66.7339, 176.9411],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  48\n",
      "    Loss:  tensor(89443.4453, grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(119.1181, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([217.,   0.,  94.,  22., 329.]) Y Pred:  tensor([122.9017, 213.0377, 160.1744,  77.8200, 162.5399],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "EPOCH:  49\n",
      "    Loss:  tensor(1311048., grad_fn=<MseLossBackward>)\n",
      "    MAE:  tensor(336.2134, grad_fn=<MeanBackward0>)\n",
      "    Y:  tensor([ 127., 1276.,  855.,  129.,  296.]) Y Pred:  tensor([196.2695, 313.6108, 238.9849, 115.8132, 275.7934],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_vals = 5\n",
    "n_dim = 5\n",
    "lr = 1e-4\n",
    "theta_lr = 10000000000000\n",
    "thetas = [.5] * n_vals\n",
    "batch_size = 10\n",
    "model = QuantumCicuitNet(n_vals, n_dim, batch_size)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "losses = []\n",
    "\n",
    "for i in range(0, 50):\n",
    "\n",
    "    print(\"EPOCH: \", i)\n",
    "\n",
    "    batch_obs = random.sample(range(0, len(X)), 5)\n",
    "\n",
    "    x = np.reshape(np.array([list(i)[0:5] for i in X[batch_obs]]), (1,1,5,5))\n",
    "    # print(x)\n",
    "    Y = torch.tensor(y.detach().numpy()[batch_obs])\n",
    "\n",
    "    y_pred = model(torch.tensor(x, dtype = torch.float32), thetas)\n",
    "    # print(\"    Gradient: \", grad)\n",
    "    loss = criterion(y_pred, Y)\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(\"    Loss: \", loss)\n",
    "    print(\"    MAE: \", mae(y_pred, Y))\n",
    "    print(\"    Y: \", Y, \"Y Pred: \", y_pred)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # thetas = [i.item() for i in list(torch.tensor(thetas) + (grad * theta_lr))]\n",
    "\n",
    "    # print(\"    Thetas: \", torch.tensor(thetas) + grad * theta_lr)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5000000000000047,\n",
       " 0.5000000000000073,\n",
       " 0.5000000000000095,\n",
       " 0.5000000000000062,\n",
       " 0.5000000000000069]"
      ]
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbde2d3b250>]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"259.116562pt\" version=\"1.1\" viewBox=\"0 0 372.103125 259.116562\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-03-23T22:57:44.130734</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 259.116562 \nL 372.103125 259.116562 \nL 372.103125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 30.103125 235.238437 \nL 364.903125 235.238437 \nL 364.903125 17.798437 \nL 30.103125 17.798437 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mb807b75199\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#mb807b75199\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(42.140057 249.836875)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"107.436335\" xlink:href=\"#mb807b75199\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <g transform=\"translate(101.073835 249.836875)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"169.551362\" xlink:href=\"#mb807b75199\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 20 -->\n      <g transform=\"translate(163.188862 249.836875)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"231.66639\" xlink:href=\"#mb807b75199\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 30 -->\n      <g transform=\"translate(225.30389 249.836875)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"293.781418\" xlink:href=\"#mb807b75199\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 40 -->\n      <g transform=\"translate(287.418918 249.836875)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"355.896446\" xlink:href=\"#mb807b75199\" y=\"235.238437\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 50 -->\n      <g transform=\"translate(349.533946 249.836875)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m4c8821c91f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c8821c91f\" y=\"228.4136\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0.0 -->\n      <g transform=\"translate(7.2 232.212819)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c8821c91f\" y=\"191.079157\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.5 -->\n      <g transform=\"translate(7.2 194.878376)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c8821c91f\" y=\"153.744714\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.0 -->\n      <g transform=\"translate(7.2 157.543933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c8821c91f\" y=\"116.410271\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.5 -->\n      <g transform=\"translate(7.2 120.20949)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c8821c91f\" y=\"79.075828\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 2.0 -->\n      <g transform=\"translate(7.2 82.875047)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m4c8821c91f\" y=\"41.741385\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 2.5 -->\n      <g transform=\"translate(7.2 45.540604)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- 1e6 -->\n     <g transform=\"translate(30.103125 14.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"125.146484\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#pa2fee2665f)\" d=\"M 45.321307 155.174553 \nL 51.53281 183.793345 \nL 57.744312 208.752841 \nL 63.955815 83.973286 \nL 70.167318 159.253467 \nL 76.378821 181.770738 \nL 82.590324 196.08554 \nL 88.801826 93.887643 \nL 95.013329 27.682074 \nL 101.224832 211.15596 \nL 107.436335 205.891082 \nL 113.647837 199.135589 \nL 119.85934 195.153502 \nL 126.070843 94.625306 \nL 132.282346 112.317427 \nL 138.493849 200.391126 \nL 144.705351 225.354801 \nL 150.916854 122.283782 \nL 157.128357 180.177075 \nL 163.33986 193.238826 \nL 169.551362 206.684873 \nL 175.762865 223.793864 \nL 181.974368 183.074914 \nL 188.185871 187.061346 \nL 194.397374 205.799524 \nL 200.608876 211.263377 \nL 206.820379 204.344491 \nL 213.031882 198.583767 \nL 219.243385 194.077129 \nL 225.454888 212.744953 \nL 231.66639 182.530078 \nL 237.877893 203.561711 \nL 244.089396 177.229801 \nL 250.300899 222.590474 \nL 256.512401 177.157325 \nL 262.723904 103.408029 \nL 268.935407 115.317231 \nL 275.14691 200.170344 \nL 281.358413 188.367155 \nL 287.569915 198.290181 \nL 293.781418 203.720385 \nL 299.992921 103.581718 \nL 306.204424 193.7777 \nL 312.415926 224.991389 \nL 318.627429 218.907794 \nL 324.838932 223.121603 \nL 331.050435 218.608098 \nL 337.261938 215.758995 \nL 343.47344 221.734958 \nL 349.684943 130.519106 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 30.103125 235.238437 \nL 30.103125 17.798437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 364.903125 235.238437 \nL 364.903125 17.798437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 30.103125 235.238437 \nL 364.903125 235.238437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 30.103125 17.798437 \nL 364.903125 17.798437 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pa2fee2665f\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"17.798437\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([i for i in range(0, 50)], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-36db8fbec5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;36m961.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m154.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m905.\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m225.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1071.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "tensor([ 961.,  154.,  905.,  225., 1071.])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OrderedDict([('conv2d.weight',\n",
       "              tensor([[[[ 8.4573,  1.8407,  8.3353],\n",
       "                        [ 7.5021,  1.9258,  5.1145],\n",
       "                        [13.3202,  3.4298,  6.5715]]]])),\n",
       "             ('linear.weight',\n",
       "              tensor([[ 7.4781e-01,  2.7693e+00,  5.4411e-01,  8.4746e-01,  1.8755e+00,\n",
       "                        3.2299e+00,  6.1290e-01,  5.8337e-01,  1.4610e+00,  2.8425e+00,\n",
       "                        1.1034e+00,  7.8914e-01,  1.5820e+00,  5.9178e-01,  2.6637e-01,\n",
       "                        4.8629e-02],\n",
       "                      [ 1.7708e+00,  5.0708e+00,  1.0794e+00,  1.0516e+00,  5.2003e+00,\n",
       "                        8.6700e+00,  1.9259e+00,  1.1239e+00,  5.6716e+00,  8.5306e+00,\n",
       "                        9.9454e-01,  6.9384e-01,  3.2437e+00,  2.3921e+00,  2.1727e-01,\n",
       "                       -4.2852e-02],\n",
       "                      [ 1.0250e+00,  3.2627e+00,  8.4865e-01,  1.0349e+00,  3.6238e+00,\n",
       "                        5.8725e+00,  1.8316e+00,  1.4129e+00,  3.4644e+00,  6.4431e+00,\n",
       "                        1.3266e+00,  1.1690e+00,  2.6215e+00,  2.0831e+00,  3.2578e-01,\n",
       "                        1.0930e-01],\n",
       "                      [-6.3549e-03,  1.8056e+00,  4.6150e-01,  2.9552e-01,  1.3239e+00,\n",
       "                        1.9096e+00,  5.5917e-01,  1.1975e-01,  1.4361e+00,  1.8045e+00,\n",
       "                        1.3113e-01,  3.7174e-01,  9.6136e-01,  4.0229e-01,  2.5238e-01,\n",
       "                        1.6030e-01],\n",
       "                      [ 9.3832e-01,  2.7581e+00,  6.8582e-01,  6.8122e-01,  2.9502e+00,\n",
       "                        4.5501e+00,  1.0865e+00,  8.8930e-01,  3.2054e+00,  5.1430e+00,\n",
       "                        1.3456e+00,  7.0808e-01,  1.5874e+00,  1.3136e+00,  3.8103e-01,\n",
       "                       -9.2403e-02]])),\n",
       "             ('linear.bias',\n",
       "              tensor([2.7819, 2.8597, 2.2301, 1.8952, 1.6662]))])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 127., 1276.,  855.,  129.,  296.])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "new_model = QuantumCicuitNet(n_vals, n_dim, 10)\n",
    "new_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [i for i in range(0, 100)]\n",
    "\n",
    "obs = [lst[i:i + 5] for i in range(0, len(lst), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4],\n",
       " [5, 6, 7, 8, 9],\n",
       " [10, 11, 12, 13, 14],\n",
       " [15, 16, 17, 18, 19],\n",
       " [20, 21, 22, 23, 24],\n",
       " [25, 26, 27, 28, 29],\n",
       " [30, 31, 32, 33, 34],\n",
       " [35, 36, 37, 38, 39],\n",
       " [40, 41, 42, 43, 44],\n",
       " [45, 46, 47, 48, 49],\n",
       " [50, 51, 52, 53, 54],\n",
       " [55, 56, 57, 58, 59],\n",
       " [60, 61, 62, 63, 64],\n",
       " [65, 66, 67, 68, 69],\n",
       " [70, 71, 72, 73, 74],\n",
       " [75, 76, 77, 78, 79],\n",
       " [80, 81, 82, 83, 84],\n",
       " [85, 86, 87, 88, 89],\n",
       " [90, 91, 92, 93, 94],\n",
       " [95, 96, 97, 98, 99]]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5, 0.5, 0.5, 0.5, 0.5]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([133.3662, 334.1721, 251.6455,  80.9092, 199.2657],\n       grad_fn=<AddBackward0>) tensor([ 961.,  154.,  905.,  225., 1071.])\ntensor([114.3336, 276.3468, 209.2206,  68.1378, 165.7147],\n       grad_fn=<AddBackward0>) tensor([ 337.,  204.,  320.,   61., 2063.])\ntensor([122.1673, 314.7249, 231.9775,  75.8172, 182.4469],\n       grad_fn=<AddBackward0>) tensor([125., 170., 378., 318., 777.])\ntensor([127.8512, 330.9451, 243.5501,  79.6560, 191.2955],\n       grad_fn=<AddBackward0>) tensor([201., 427., 187., 133., 304.])\ntensor([ 89.0033, 235.8915, 176.6821,  56.5047, 138.3339],\n       grad_fn=<AddBackward0>) tensor([479., 464., 163., 772.,  57.])\ntensor([115.3924, 297.6278, 217.9004,  72.4820, 171.8547],\n       grad_fn=<AddBackward0>) tensor([219., 211.,  80., 267., 307.])\ntensor([108.1415, 276.7369, 205.6487,  66.8332, 163.2672],\n       grad_fn=<AddBackward0>) tensor([ 198., 1048.,  229.,  413.,  460.])\ntensor([115.6569, 299.0531, 218.2085,  72.7238, 172.6267],\n       grad_fn=<AddBackward0>) tensor([121., 255., 147., 136., 157.])\ntensor([ 74.0686, 155.7268, 127.4618,  39.9603,  98.2902],\n       grad_fn=<AddBackward0>) tensor([268., 295., 342., 780., 494.])\ntensor([110.1999, 286.1785, 218.2680,  67.8041, 171.2701],\n       grad_fn=<AddBackward0>) tensor([ 222.,  802., 1507., 1634., 1183.])\ntensor([165.0218, 415.5165, 312.2123, 100.9616, 244.1925],\n       grad_fn=<AddBackward0>) tensor([ 522.,  281.,  943., 1354.,  984.])\ntensor([139.2689, 355.9733, 263.0907,  86.5639, 207.1507],\n       grad_fn=<AddBackward0>) tensor([ 619., 1175.,  897., 1201., 2672.])\ntensor([154.4567, 390.4645, 291.9823,  94.7848, 228.3324],\n       grad_fn=<AddBackward0>) tensor([ 568., 1363.,  830.,  436., 1862.])\ntensor([160.9451, 406.4395, 304.3611,  99.0638, 237.9205],\n       grad_fn=<AddBackward0>) tensor([2042.,  350.,  951.,  481.,  556.])\ntensor([149.6871, 377.8262, 282.6070,  91.9106, 220.9664],\n       grad_fn=<AddBackward0>) tensor([ 592.,  515.,  694.,  344., 1777.])\ntensor([144.9198, 369.9887, 275.9193,  89.4369, 215.7895],\n       grad_fn=<AddBackward0>) tensor([1974., 1581.,  519., 1025., 1053.])\ntensor([154.0983, 392.1865, 291.2216,  95.4158, 228.4753],\n       grad_fn=<AddBackward0>) tensor([ 719.,  708., 1072.,  440.,  861.])\ntensor([148.1007, 376.9681, 281.7018,  90.9472, 221.8306],\n       grad_fn=<AddBackward0>) tensor([ 723.,  138., 1419.,  542.,  287.])\ntensor([146.6561, 385.5671, 283.6249,  91.8933, 222.7285],\n       grad_fn=<AddBackward0>) tensor([ 996.,  511., 1119., 3623.,  567.])\ntensor([135.0859, 353.7408, 258.5236,  84.2999, 204.6862],\n       grad_fn=<AddBackward0>) tensor([4955.,  500.,  278.,  392.,  225.])\n"
     ]
    }
   ],
   "source": [
    "for subset in obs:\n",
    "\n",
    "    x = np.reshape(np.array([list(i)[0:5] for i in X[subset]]), (1,1,5,5))\n",
    "    Y = torch.tensor(y.detach().numpy()[subset])\n",
    "\n",
    "    y_pred = model(torch.tensor(x, dtype = torch.float32), thetas)\n",
    "    print(y_pred, Y)\n",
    "    # loss = criterion(y_pred, Y)\n",
    "    # losses.append(loss)\n",
    "\n",
    "    # print(\"    Loss: \", loss)\n",
    "    # print(\"    MAE: \", mae(y_pred, Y))\n",
    "\n",
    "\n",
    "    # optimizer.zero_grad()\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "\n",
    "    # thetas = [i.item() for i in list(torch.tensor(thetas) + (grad * lr))]\n",
    "\n",
    "    # print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}